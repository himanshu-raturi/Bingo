{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaOn1Go7NCSm"
      },
      "source": [
        "# Byte Language Model\n",
        "\n",
        "1. Copy this colab notebook into your google drive.\n",
        "2. You need background knowledge for [Python](https://www.python.org/) and [NumPy](https://numpy.org/).\n",
        "3. Run the cells yourself and tweak the code so that the byte-wise language model works as expected. Note that the current implementation does not perform any smoothing and thus leading to `inf` perplexity.\n",
        "4. Save this colab notebook as a pdf via **Print** in the file menu and submit it to https://edu-portal.naist.jp/ under **NLP #3 & #4** of **2024 NAIST 4102 NLP** using the report submission portal. Please make sure that all the codes and the execution results are visible for the assessment.\n",
        "5. Due date is **January 8th, 2025 JST**.\n",
        "\n",
        "For help regarding [Colab](https://colab.research.google.com/) or any technical issues, ask our TA, Hongyu Sun <sun.hongyu.sg6@naist.ac.jp>.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "x3X8DSpYMnFR"
      },
      "outputs": [],
      "source": [
        "#@markdown Please fill in your name, student id and email address.\n",
        "\n",
        "name = 'Raturi Himanshu' #@param {type: 'string'}\n",
        "stuent_id = '2411422' #@param {type: 'string'}\n",
        "email = 'raturi.himanshu.rf4@naist.ac.jp' #@param {type: 'string'}\n",
        "\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4gtwt2NWyPN"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "We will give 70 points for fixing a bug in the following code so that it can return perplexity values correctly, i.e., finite values, not, e.g., `Inf`. Additional points, i.e., maximum of 30, are given based on the ranking of the perpelxity, i.e., lower is better, among submissions.\n",
        "\n",
        "* We will rank by the sum of three byte-wise perplexities, not word-wise perplexities, measured on three languages, Czech, German and Chinese.\n",
        "* The scores are linearly computed among ranks. E.g., 30 is given to the submission with the lowest perplexity, 0 to those with the highest perplexity, and 15 to those in the middle of the ranks.\n",
        "* Ties are grouped together as a bin, and their scores are computed by taking the average in the bins. For example, if a bin has 3 submissions with their linearly assigned scores of 13, 14 and 15, the average, i.e., $(13 + 14 + 15) / 3 = 14$, will be credited to those three.\n",
        "* You can use any external libraries so long as you don't break APIs as documented/commented in the code. They are indicated by \"DO NOT CHANGE\" etc.\n",
        "* If you tune any hyperparameters, please leave your experiments in this colab, e.g., your code and results. When tuning hyperparameters, do not tune them on the final test data, but use the development data.\n",
        "\n",
        "### Penalties\n",
        "\n",
        "The byte-wise perplexity will be penalyzed by adding a penaty term when the cumulative product of sum of probabilities is not closer to 1 using the following formula:\n",
        "\n",
        "$penalty = \\max(\\operatorname{abs}(1 - \\prod_{t=1}^{T} \\sum_{y=0}^{255} p(y | x_{<t})) - 0.001, 0) \\times 100$\n",
        "\n",
        "where $x_t$ is a byte in a file $\\boldsymbol{x}$ at position $t$. Note that your language model must be probabilitistic in that sum over all byte values must be equal to one given any histories.\n",
        "\n",
        "### Extras\n",
        "\n",
        "Those who tried \"unique methods\" will be given at most 10 points in addition to the base and ranking points. The uniqueness is determined whether a submission employs a smoothing method other students have not tried. However, maximum 10 points will be deducted when violating rules, e.g., changing part of the codes/APIs which should not be modified, or tuning hyperparameters on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfi4Sd4kT3Kc"
      },
      "source": [
        "## Download datasets\n",
        "\n",
        "We will down load the datasets to train and test your language models. The dataset is extracted from [WMT 2024 Shared Task](https://www2.statmt.org/wmt24/translation-task.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khLcyFcco2H5",
        "outputId": "da3edb1d-7fc6-4497-9356-a7e9204eda82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15P-8w5Y90qG3GOoyKzq4PrKUjN6LDFpV\n",
            "From (redirected): https://drive.google.com/uc?id=15P-8w5Y90qG3GOoyKzq4PrKUjN6LDFpV&confirm=t&uuid=94f2d80e-89e9-4ba4-893c-3fa546b38b0e\n",
            "To: /content/byte-language-model-2024.de.zip\n",
            "100% 27.0M/27.0M [00:00<00:00, 30.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wVaCzs2fOT7ZWfwrFj_duI0yKU_4Qcqq\n",
            "To: /content/byte-language-model-2024.hi.zip\n",
            "100% 673k/673k [00:00<00:00, 36.7MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1C-d9-J_BdFd7ADoqre5YYkYkdq0p-Dfh\n",
            "From (redirected): https://drive.google.com/uc?id=1C-d9-J_BdFd7ADoqre5YYkYkdq0p-Dfh&confirm=t&uuid=7037d76f-fa55-48ab-87f6-2c2fe904f178\n",
            "To: /content/byte-language-model-2024.ru.zip\n",
            "100% 29.8M/29.8M [00:00<00:00, 58.4MB/s]\n",
            "Archive:  byte-language-model-2024.de.zip\n",
            "  inflating: byte-language-model-2024.de/news-commentary-v18.de.txt  \n",
            "  inflating: byte-language-model-2024.de/wmttest2023.de.txt  \n",
            "  inflating: byte-language-model-2024.de/wmttest2024.de.txt  \n",
            "Archive:  byte-language-model-2024.hi.zip\n",
            "  inflating: byte-language-model-2024.hi/news-commentary-v18.hi.txt  \n",
            "  inflating: byte-language-model-2024.hi/wmttest2023.hi.txt  \n",
            "  inflating: byte-language-model-2024.hi/wmttest2024.hi.txt  \n",
            "Archive:  byte-language-model-2024.ru.zip\n",
            "  inflating: byte-language-model-2024.ru/news-commentary-v18.ru.txt  \n",
            "  inflating: byte-language-model-2024.ru/wmttest2023.ru.txt  \n",
            "  inflating: byte-language-model-2024.ru/wmttest2024.ru.txt  \n"
          ]
        }
      ],
      "source": [
        "# Download the file to \"/content\" directory.\n",
        "!gdown 15P-8w5Y90qG3GOoyKzq4PrKUjN6LDFpV\n",
        "!gdown 1wVaCzs2fOT7ZWfwrFj_duI0yKU_4Qcqq\n",
        "!gdown 1C-d9-J_BdFd7ADoqre5YYkYkdq0p-Dfh\n",
        "\n",
        "# Unzip it.\n",
        "!unzip -o byte-language-model-2024.de.zip\n",
        "!unzip -o byte-language-model-2024.hi.zip\n",
        "!unzip -o byte-language-model-2024.ru.zip\n",
        "\n",
        "# Create smaller training datasets comprising 3000 sentences for development\n",
        "# purposes.\n",
        "! head -3000 byte-language-model-2024.de/news-commentary-v18.de.txt > byte-language-model-2024.de/news-commentary-v18-small.de.txt\n",
        "! head -3000 byte-language-model-2024.hi/news-commentary-v18.hi.txt > byte-language-model-2024.hi/news-commentary-v18-small.hi.txt\n",
        "! head -3000 byte-language-model-2024.ru/news-commentary-v18.ru.txt > byte-language-model-2024.ru/news-commentary-v18-small.ru.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9pYXO_ucNjq"
      },
      "source": [
        "## Verifies the extracted files\n",
        "\n",
        "We will compute the md5 checksums to make sure that you are using the correct files. You will observe the following outputs:\n",
        "\n",
        "```bash\n",
        "c5cf2b0f973eccb12e45a56039d44f99  byte-language-model-2024.de/news-commentary-v18.de.txt\n",
        "383597f984448acb4d4d09c839055d26  byte-language-model-2024.de/news-commentary-v18-small.de.txt\n",
        "b58c3944859f9df96271e1f56d5a97e7  byte-language-model-2024.de/wmttest2023.de.txt\n",
        "9efc37e56a73014c2e17856a0d09b4bd  byte-language-model-2024.de/wmttest2024.de.txt\n",
        "acd6b1f3391ae49bb7ab7df0eae2ac78  byte-language-model-2024.hi/news-commentary-v18.hi.txt\n",
        "6971694001b2ae62e01c8502cd61bba1  byte-language-model-2024.hi/news-commentary-v18-small.hi.txt\n",
        "f410731105f7cd313148069f096e34c3  byte-language-model-2024.hi/wmttest2023.hi.txt\n",
        "70c511e65e6e10de9daccc5e0a2b103f  byte-language-model-2024.hi/wmttest2024.hi.txt\n",
        "2516f578eea6428dd3e6d9a0c2135bf8  byte-language-model-2024.ru/news-commentary-v18.ru.txt\n",
        "f976485f7baaf7a66159f23ab3c69025  byte-language-model-2024.ru/news-commentary-v18-small.ru.txt\n",
        "7620468544b08798fb5172e6406826c7  byte-language-model-2024.ru/wmttest2023.ru.txt\n",
        "ec00fc9496a3e1077baf328c82dc2421  byte-language-model-2024.ru/wmttest2024.ru.txt\n",
        "```\n",
        "\n",
        "Note that we will use `news-commentary-v18.{de,hi,ru}.txt` for training and `wmttest2024.{de,hi,ru}.txt` for the final testing. `wmttest2023.{de,hi,ru}.txt` will be used for your development purposes, e.g., debugging or fine tuning hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcoWlZ33cNF2",
        "outputId": "f0fda833-d477-4177-8c80-902d057be5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c5cf2b0f973eccb12e45a56039d44f99  byte-language-model-2024.de/news-commentary-v18.de.txt\n",
            "383597f984448acb4d4d09c839055d26  byte-language-model-2024.de/news-commentary-v18-small.de.txt\n",
            "b58c3944859f9df96271e1f56d5a97e7  byte-language-model-2024.de/wmttest2023.de.txt\n",
            "9efc37e56a73014c2e17856a0d09b4bd  byte-language-model-2024.de/wmttest2024.de.txt\n",
            "acd6b1f3391ae49bb7ab7df0eae2ac78  byte-language-model-2024.hi/news-commentary-v18.hi.txt\n",
            "6971694001b2ae62e01c8502cd61bba1  byte-language-model-2024.hi/news-commentary-v18-small.hi.txt\n",
            "f410731105f7cd313148069f096e34c3  byte-language-model-2024.hi/wmttest2023.hi.txt\n",
            "70c511e65e6e10de9daccc5e0a2b103f  byte-language-model-2024.hi/wmttest2024.hi.txt\n",
            "2516f578eea6428dd3e6d9a0c2135bf8  byte-language-model-2024.ru/news-commentary-v18.ru.txt\n",
            "f976485f7baaf7a66159f23ab3c69025  byte-language-model-2024.ru/news-commentary-v18-small.ru.txt\n",
            "7620468544b08798fb5172e6406826c7  byte-language-model-2024.ru/wmttest2023.ru.txt\n",
            "ec00fc9496a3e1077baf328c82dc2421  byte-language-model-2024.ru/wmttest2024.ru.txt\n"
          ]
        }
      ],
      "source": [
        "# Runs md5sum for the unzipped file.\n",
        "# please make sure tat the hash codes are the same.\n",
        "\n",
        "!md5sum byte-language-model-2024.*/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbM4x0Ht4JtZ"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "Adds necessary imports here if you want to use additional libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EwlCwsiGsFRF"
      },
      "outputs": [],
      "source": [
        "# Import libraries used in this colab.\n",
        "import collections\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "from google.colab import files\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wt8sYGWUIqy"
      },
      "source": [
        "## Language model implementation\n",
        "\n",
        "`ByteLM` is a language model class which loads a training data, estimate parameters and test it on a file to compute byet-wise perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oMcNDJo-0eaE"
      },
      "outputs": [],
      "source": [
        "class ByteLM:\n",
        "  \"\"\"Byte language model.\n",
        "\n",
        "  This is a very naive language model, in which byte-wise ngram probabilities\n",
        "  are estimated by maximum-likelihood without considering an issue of\n",
        "  out-of-vocabulary.\n",
        "\n",
        "  You may want to tweak `__init__`, `initial_state` and `logprob` methods to\n",
        "  alleviate the problem. However, do not change `perplexity` for a fair\n",
        "  comparison with other models implemented by other students. When changing part\n",
        "  of the codes, please try to make it readable by using appropriate variable\n",
        "  names or adding comments. Feel free to add additional methods, if necessary.\n",
        "\n",
        "  Usages:\n",
        "    ```python\n",
        "    lm = ByteLM(path/to/train/data)\n",
        "    perplexity, prob = lm.perplexity(path/to/test/data)\n",
        "    ```\n",
        "  \"\"\"\n",
        "\n",
        "  # DO NOT CHANGE BOS VALUE.\n",
        "  # 0 will never appear in a text, thus, used it as a special symbol for\n",
        "  # a beginning-of-sentence symbol, i.e., BOS.\n",
        "  BOS: int = 0\n",
        "\n",
        "  def __init__(self, filename: str, order: int=3, smoothing: float=0.1) -> None:\n",
        "    \"\"\"Initializes `ByteLM`.\n",
        "\n",
        "    You can change the arguments for this method if necessary, e.g., adding\n",
        "    hyperparameters to this model.\n",
        "\n",
        "    Args:\n",
        "      filename: str, text file to train this language model.\n",
        "      order: int, the n-gram order that should be greater than 1.\n",
        "    \"\"\"\n",
        "    if order <= 1:\n",
        "      raise ValueError(f'`order` must be greater than 1: {order}')\n",
        "    self.order = order\n",
        "    self.smoothing = smoothing\n",
        "    # You can try revise the code in this method to fix a bug and to achieve\n",
        "    # lower perplexity.\n",
        "\n",
        "    # Collect n-gram counts. The dictionary comprises a key of tuple of\n",
        "    # integers, i.e., (n-1)-gram, and its associated value of 256-dimensonal\n",
        "    # vector, i.e., counts for the following chars.\n",
        "    ngram_counts = collections.defaultdict(lambda: np.zeros([256]))\n",
        "    with open(filename, 'br') as f:\n",
        "      for line in f:  # read as a byte string.\n",
        "        buffer = [self.BOS] + list(line)  # `buffer` is now a list of integers.\n",
        "        for n in range(1, self.order + 1):\n",
        "          for i in range(len(buffer) - n + 1):\n",
        "            ngram = buffer[i:i + n]\n",
        "            ngram_counts[tuple(ngram[:-1])][ngram[-1]] += 1\n",
        "\n",
        "    # Maximum likelihood estimate for language model.\n",
        "    self.ngrams: Dict[Tuple[int], np.ndarray] = {}\n",
        "    for context, counts in ngram_counts.items():\n",
        "      smoothed_counts = counts + self.smoothing\n",
        "      probs = smoothed_counts / np.sum(smoothed_counts)\n",
        "      # Computes log probabilities, but assigns -inf for zero probabilities.\n",
        "      log_probs = np.log(probs)\n",
        "      self.ngrams[context] = log_probs\n",
        "\n",
        "  def initial_state(self) -> Any:\n",
        "    \"\"\"Returns an initial state for language model computation.\n",
        "\n",
        "    You can change the code in this method, but keep the API, e.g, input\n",
        "    arguments, so that `perplexity()` method works as expected.\n",
        "\n",
        "    Returns:\n",
        "      A state representation for log probabilities computation.\n",
        "    \"\"\"\n",
        "    # You can revise the code here for lower perplexity.\n",
        "    return [self.BOS]\n",
        "\n",
        "  def logprob(self, state: Any, x: int) -> Tuple[np.ndarray, Any]:\n",
        "    \"\"\"Returns log probabilities for the current input byte.\n",
        "\n",
        "    You can change the code in this method, but keep the API, e.g, input\n",
        "    arguments, so that `perplexity()` method works as expected.\n",
        "\n",
        "    Args:\n",
        "      state: A state to compute log probability.\n",
        "      x: int, the current byte to compute `p(y | state, x)`.\n",
        "    Returns:\n",
        "      A pair of (log_probs, next_state) where `log_probs` is `np.ndarray` of log\n",
        "      probabilities p(y | state, x) of all bytes y, and `next_state` is a new\n",
        "      state for the next log probability computation with a new input. Note that\n",
        "      `log_probs[y]` is equal to `log p(y | state, x)`,\n",
        "      `log_probs.shape == (256,)`, `np.exp(log_probs) >= 0` and\n",
        "      `np.sum(np.exp(log_probs)) == 1`.\n",
        "    \"\"\"\n",
        "    # You many want to revise the code in this method to achieve lower\n",
        "    # perplexity.\n",
        "\n",
        "    # Backoff to lower order when necessary.\n",
        "    state = (state + [x])[-self.order + 1:]\n",
        "    for i in range(len(state), 0, -1):\n",
        "       context = state[-i:]\n",
        "       assert len(context) < self.order\n",
        "       ret = self.ngrams.get(tuple(context), None)\n",
        "       if ret is not None:\n",
        "         return ret, context\n",
        "\n",
        "    # Backoff to unigram.\n",
        "    ret = self.ngrams.get((), None)\n",
        "    assert ret is not None\n",
        "    return ret, []\n",
        "\n",
        "  def perplexity(self, filename: str) -> Tuple[float, float]:\n",
        "    \"\"\"Computes perplexity for text data.\n",
        "\n",
        "    DO NOT CHANGE THE API OR CODE IN THIS METHOD.\n",
        "\n",
        "    Args:\n",
        "      filename: str, text file to compute perplexity.\n",
        "    Returns:\n",
        "      A pair (perplexity, prob) where `perplexity` is the perplexity computed\n",
        "      for `filename`. `prob` is the cumulative product of probabilities of all\n",
        "      the bytes in `filename` to verify that this language model is\n",
        "      probabilistic or not. `prob` should be close to 1, otherwise, this is not\n",
        "      a language model.\n",
        "    \"\"\"\n",
        "    # Cumulative log_prob for perplexity computation.\n",
        "    cumulative_log_prob = 0.0\n",
        "    # Verify the distribution so that this language model is probabilistic.\n",
        "    prob = 1.0\n",
        "    # Total number of bytes.\n",
        "    total_bytes = 0\n",
        "    with open(filename, 'br') as f:\n",
        "      for line in f:\n",
        "        state = self.initial_state()\n",
        "        prev_x = self.BOS\n",
        "        for x in line:\n",
        "          log_probs, state = self.logprob(state, prev_x)\n",
        "          assert log_probs.size == 256, f\"expected 256, got: {log_probs.size}\"\n",
        "          cumulative_log_prob += log_probs[x]\n",
        "\n",
        "          probs = np.exp(log_probs)\n",
        "          assert (probs >= 0).all(), \"expected greater than or equal to zero.\"\n",
        "          prob *= np.sum(probs)  # Sum of `probs` should be close to 1.\n",
        "\n",
        "          prev_x = x\n",
        "\n",
        "        total_bytes += len(line)\n",
        "\n",
        "    return np.exp(-cumulative_log_prob / total_bytes), prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSbRCBqpToid"
      },
      "source": [
        "## Run and report perplexity\n",
        "\n",
        "Please run the following code block to report the final perplexity of three language models. You can change the hyperparameters to the language model, e.g., additional arguments for constructing three models, `model_de`, `model_hi` and `model_ru`. However, do not change the rest of the code for a fair comparison with others.\n",
        "\n",
        "We will rank your \"adjusted\" perplexity results from three language models after adding penalty terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIhSOEdJzqgd",
        "outputId": "c2c6f3ba-335a-407a-db2b-673c5f61b39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c5cf2b0f973eccb12e45a56039d44f99  byte-language-model-2024.de/news-commentary-v18.de.txt\n",
            "383597f984448acb4d4d09c839055d26  byte-language-model-2024.de/news-commentary-v18-small.de.txt\n",
            "b58c3944859f9df96271e1f56d5a97e7  byte-language-model-2024.de/wmttest2023.de.txt\n",
            "9efc37e56a73014c2e17856a0d09b4bd  byte-language-model-2024.de/wmttest2024.de.txt\n",
            "acd6b1f3391ae49bb7ab7df0eae2ac78  byte-language-model-2024.hi/news-commentary-v18.hi.txt\n",
            "6971694001b2ae62e01c8502cd61bba1  byte-language-model-2024.hi/news-commentary-v18-small.hi.txt\n",
            "f410731105f7cd313148069f096e34c3  byte-language-model-2024.hi/wmttest2023.hi.txt\n",
            "70c511e65e6e10de9daccc5e0a2b103f  byte-language-model-2024.hi/wmttest2024.hi.txt\n",
            "2516f578eea6428dd3e6d9a0c2135bf8  byte-language-model-2024.ru/news-commentary-v18.ru.txt\n",
            "f976485f7baaf7a66159f23ab3c69025  byte-language-model-2024.ru/news-commentary-v18-small.ru.txt\n",
            "7620468544b08798fb5172e6406826c7  byte-language-model-2024.ru/wmttest2023.ru.txt\n",
            "ec00fc9496a3e1077baf328c82dc2421  byte-language-model-2024.ru/wmttest2024.ru.txt\n",
            "de perplexity: 8.901993619329245 prob: 0.9999999999909398 penalty: 0.0\n",
            "hi perplexity: 3.5138582078420324 prob: 0.9999999999437356 penalty: 0.0\n",
            "ru perplexity: 4.603550517545005 prob: 1.0000000000055715 penalty: 0.0\n",
            "total perplexity: 17.019402344716283\n",
            "adjusted: 17.0194\n"
          ]
        }
      ],
      "source": [
        "# Computes md5 hash to make sure that the correct datasets are used for training\n",
        "# and testing.\n",
        "!md5sum byte-language-model-2024.*/*\n",
        "\n",
        "# Train a Czech language model using German data. Note that you can modify the\n",
        "# arguments to `ByteLM` to set hyperparameters to the language model.\n",
        "model_de = ByteLM(\"byte-language-model-2024.de/news-commentary-v18.de.txt\")\n",
        "\n",
        "# Train a Hindi language model using German data. Note that you can modify the\n",
        "# arguments to `ByteLM` to set hyperparameters to the language model.\n",
        "model_hi = ByteLM(\"byte-language-model-2024.hi/news-commentary-v18.hi.txt\")\n",
        "\n",
        "# Train a Russian language model using Chinese data. Note that you can modify\n",
        "# the arguments to `ByteLM` to set hyperparameters to the language model.\n",
        "model_ru = ByteLM(\"byte-language-model-2024.ru/news-commentary-v18.ru.txt\")\n",
        "\n",
        "# DO NOT CHANGE THE FOLLOWING CODES FOR FAIR COMPARISON.\n",
        "\n",
        "# Testing on Czech data using the language model trained by German data.\n",
        "perp_de, prob_de = model_de.perplexity(\"byte-language-model-2024.de/wmttest2024.de.txt\")\n",
        "\n",
        "# Testing on German data using the language model trained by Hindi data.\n",
        "perp_hi, prob_hi = model_hi.perplexity(\"byte-language-model-2024.hi/wmttest2024.hi.txt\")\n",
        "\n",
        "# Testing on Chinese data using the language model trained by Russain data.\n",
        "perp_ru, prob_ru = model_ru.perplexity(\"byte-language-model-2024.ru/wmttest2024.ru.txt\")\n",
        "\n",
        "# Computes total perplexity from three languages.\n",
        "perp = perp_de + perp_hi + perp_ru\n",
        "\n",
        "# Computes penalties and simply sums them.\n",
        "penalty_de = np.maximum(np.abs(1 - prob_de) - 0.001, 0) * 100\n",
        "penalty_hi = np.maximum(np.abs(1 - prob_hi) - 0.001, 0) * 100\n",
        "penalty_ru = np.maximum(np.abs(1 - prob_ru) - 0.001, 0) * 100\n",
        "adjusted_perp = perp + penalty_de + penalty_hi + penalty_ru\n",
        "\n",
        "# Print out computation results. \"adjusted perplexity\" is used for ranking.\n",
        "print(f\"de perplexity: {perp_de} prob: {prob_de} penalty: {penalty_de}\")\n",
        "print(f\"hi perplexity: {perp_hi} prob: {prob_hi} penalty: {penalty_hi}\")\n",
        "print(f\"ru perplexity: {perp_ru} prob: {prob_ru} penalty: {penalty_ru}\")\n",
        "print(f\"total perplexity: {perp}\")\n",
        "print(f\"adjusted: {adjusted_perp:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg8opr-gUVm2"
      },
      "source": [
        "## Add your codes if necessary\n",
        "\n",
        "You can add arbitrary codes here, e.g., running experiments on smaller training data, i.e., `byte-language-model-2024.de/news-commentary-v18-small.de.txt` and/or `byte-language-model-2024.hi/news-commentary-v18-small.hi.txt`, together with development data, `byte-language-model-2024.de/wmttest2023.de.txt` and/or `byte-language-model-2024.hi/wmttest2023.hi.txt`. You can easily add your code by clicking `+ Code` at the top of this notebook, near the menu bar.\n",
        "\n",
        "When you tweak any hyperparameters of your model, you may keep some code run results as a justification of the choices, e.g., run results on the development datasets. However, do not tune any hyperparameters on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jKTAPE7J6CY3"
      },
      "outputs": [],
      "source": [
        "# You can add your code here for your testing purposes, e.g., runs on\n",
        "# development data to tweak your codes in `ByteLM` or find hyperparameters.\n",
        "# However, do not tune on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "h2bEakKq296q"
      },
      "outputs": [],
      "source": [
        "class ByteLM:\n",
        "  BOS: int = 0\n",
        "\n",
        "  def __init__(self, filename: str, order: int=3, smoothing: float=1.0) -> None:\n",
        "    if order <= 1:\n",
        "      raise ValueError(f'`order` must be greater than 1: {order}')\n",
        "    self.order = order\n",
        "    self.smoothing = smoothing\n",
        "\n",
        "    ngram_counts = collections.defaultdict(lambda: np.zeros([256]))\n",
        "    with open(filename, 'br') as f:\n",
        "      for line in f:  # read as a byte string.\n",
        "        buffer = [self.BOS] + list(line)  # `buffer` is now a list of integers.\n",
        "        for n in range(1, self.order + 1):\n",
        "          for i in range(len(buffer) - n + 1):\n",
        "            ngram = buffer[i:i + n]\n",
        "            ngram_counts[tuple(ngram[:-1])][ngram[-1]] += 1\n",
        "\n",
        "    self.ngrams: Dict[Tuple[int], np.ndarray] = {}\n",
        "    for context, counts in ngram_counts.items():\n",
        "      smoothed_counts = counts + self.smoothing\n",
        "      probs = smoothed_counts / np.sum(smoothed_counts)\n",
        "      log_probs = np.log(probs)\n",
        "      self.ngrams[context] = log_probs\n",
        "\n",
        "  def initial_state(self) -> Any:\n",
        "    return []\n",
        "\n",
        "  def logprob(self, state: Any, x: int) -> Tuple[np.ndarray, Any]:\n",
        "    state = (state + [x])[-self.order + 1:]\n",
        "    for i in range(len(state), 0, -1):\n",
        "       context = state[-i:]\n",
        "       assert len(context) < self.order\n",
        "       ret = self.ngrams.get(tuple(context), None)\n",
        "       if ret is not None:\n",
        "         return ret, context\n",
        "\n",
        "    # Backoff to unigram.\n",
        "    ret = self.ngrams.get((), None)\n",
        "    assert ret is not None\n",
        "    return ret, []\n",
        "\n",
        "  def perplexity(self, filename: str) -> Tuple[float, float]:\n",
        "    cumulative_log_prob = 0.0\n",
        "    prob = 1.0\n",
        "    total_bytes = 0\n",
        "    with open(filename, 'br') as f:\n",
        "      for line in f:\n",
        "        state = self.initial_state()\n",
        "        prev_x = self.BOS\n",
        "        for x in line:\n",
        "          log_probs, state = self.logprob(state, prev_x)\n",
        "          assert log_probs.size == 256, f\"expected 256, got: {log_probs.size}\"\n",
        "          cumulative_log_prob += log_probs[x]\n",
        "\n",
        "          probs = np.exp(log_probs)\n",
        "          assert (probs >= 0).all(), \"expected greater than or equal to zero.\"\n",
        "          prob *= np.sum(probs)  # Sum of `probs` should be close to 1.\n",
        "\n",
        "          prev_x = x\n",
        "\n",
        "        total_bytes += len(line)\n",
        "\n",
        "    return np.exp(-cumulative_log_prob / total_bytes), prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRRdiHBR28_w"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blyX4owY2486",
        "outputId": "22e77192-c9f5-49e6-df07-cb2f13ab11ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with smoothing value: 0.1\n",
            "Perplexity: 9.939325568502968, Probability: 0.9999999999999979\n",
            "Testing with smoothing value: 0.5\n",
            "Perplexity: 10.768516546223317, Probability: 0.9999999999990565\n",
            "Testing with smoothing value: 1.0\n",
            "Perplexity: 11.683562050535413, Probability: 0.9999999999999977\n",
            "Testing with smoothing value: 2.0\n",
            "Perplexity: 13.229887698336501, Probability: 0.9999999999960931\n",
            "Testing with smoothing value: 5.0\n",
            "Perplexity: 16.921358933195567, Probability: 0.9999999999977742\n",
            "Best smoothing value: 0.1 with perplexity: 9.939325568502968\n"
          ]
        }
      ],
      "source": [
        "# Define a range of smoothing values to test\n",
        "smoothing_values = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
        "\n",
        "# Store perplexity results for each smoothing value\n",
        "results = []\n",
        "\n",
        "for smoothing in smoothing_values:\n",
        "  print(f\"Testing with smoothing value: {smoothing}\")\n",
        "  model_de = ByteLM(\"byte-language-model-2024.de/news-commentary-v18-small.de.txt\", smoothing=smoothing)\n",
        "  perp_de, prob_de = model_de.perplexity(\"byte-language-model-2024.de/wmttest2023.de.txt\")\n",
        "  results.append((smoothing, perp_de))\n",
        "  print(f\"Perplexity: {perp_de}, Probability: {prob_de}\")\n",
        "\n",
        "# Find the best smoothing value\n",
        "best_smoothing = min(results, key=lambda x: x[1])\n",
        "print(f\"Best smoothing value: {best_smoothing[0]} with perplexity: {best_smoothing[1]}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}